Directory structure:
└── eventbrokers/
    ├── __init__.py
    ├── asyncpg.py
    ├── base.py
    ├── local.py
    ├── mqtt.py
    ├── psycopg.py
    └── redis.py

================================================
FILE: src/apscheduler/eventbrokers/__init__.py
================================================



================================================
FILE: src/apscheduler/eventbrokers/asyncpg.py
================================================
from __future__ import annotations

from collections.abc import AsyncGenerator, Mapping
from contextlib import AsyncExitStack, asynccontextmanager
from logging import Logger
from typing import TYPE_CHECKING, Any

import asyncpg
import attrs
from anyio import (
    EndOfStream,
    create_memory_object_stream,
    move_on_after,
)
from anyio.abc import TaskStatus
from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream
from asyncpg import Connection, InterfaceError
from attr.validators import instance_of

from .._events import Event
from .._exceptions import SerializationError
from .._utils import create_repr
from .base import BaseExternalEventBroker

if TYPE_CHECKING:
    from sqlalchemy.ext.asyncio import AsyncEngine


@attrs.define(eq=False, repr=False)
class AsyncpgEventBroker(BaseExternalEventBroker):
    """
    An asynchronous, asyncpg_ based event broker that uses a PostgreSQL server to
    broadcast events using its ``NOTIFY`` mechanism.

    .. _asyncpg: https://pypi.org/project/asyncpg/

    :param dsn: a libpq connection string (e.g.
        ``postgres://user:pass@host:port/dbname``)
    :param options: extra keyword arguments passed to :func:`asyncpg.connection.connect`
    :param channel: the ``NOTIFY`` channel to use
    :param max_idle_time: maximum time to let the connection go idle, before sending a
        ``SELECT 1`` query to prevent a connection timeout
    """

    dsn: str
    options: Mapping[str, Any] = attrs.field(
        factory=dict, validator=instance_of(Mapping)
    )
    channel: str = attrs.field(kw_only=True, default="apscheduler")
    max_idle_time: float = attrs.field(kw_only=True, default=10)

    _send: MemoryObjectSendStream[str] = attrs.field(init=False)

    @classmethod
    def from_async_sqla_engine(
        cls,
        engine: AsyncEngine,
        options: Mapping[str, Any] | None = None,
        **kwargs: Any,
    ) -> AsyncpgEventBroker:
        """
        Create a new asyncpg event broker from an SQLAlchemy engine.

        The engine will only be used to create the appropriate options for
        :func:`asyncpg.connection.connect`.

        :param engine: an asynchronous SQLAlchemy engine using asyncpg as the driver
        :type engine: ~sqlalchemy.ext.asyncio.AsyncEngine
        :param options: extra keyword arguments passed to
            :func:`asyncpg.connection.connect`
        :param kwargs: keyword arguments to pass to the initializer of this class
        :return: the newly created event broker

        """
        if engine.dialect.driver != "asyncpg":
            raise ValueError(
                f'The driver in the engine must be "asyncpg" (current: '
                f"{engine.dialect.driver})"
            )

        dsn = engine.url.render_as_string(hide_password=False).replace("+asyncpg", "")
        return cls(dsn, options or {}, **kwargs)

    def __repr__(self) -> str:
        return create_repr(self, "dsn")

    @property
    def _temporary_failure_exceptions(self) -> tuple[type[Exception], ...]:
        return OSError, InterfaceError

    @asynccontextmanager
    async def _connect(self) -> AsyncGenerator[asyncpg.Connection, None]:
        async for attempt in self._retry():
            with attempt:
                conn = await asyncpg.connect(self.dsn, **self.options)
                try:
                    yield conn
                finally:
                    with move_on_after(5, shield=True):
                        await conn.close(timeout=3)

    async def start(self, exit_stack: AsyncExitStack, logger: Logger) -> None:
        await super().start(exit_stack, logger)
        self._send, receive = create_memory_object_stream[str](100)
        await exit_stack.enter_async_context(self._send)
        await self._task_group.start(self._listen_notifications, receive)

    async def _listen_notifications(
        self, receive: MemoryObjectReceiveStream[str], *, task_status: TaskStatus[None]
    ) -> None:
        conn: Connection

        def listen_callback(
            connection: Connection, pid: int, channel: str, payload: str
        ) -> None:
            event = self.reconstitute_event_str(payload)
            if event is not None:
                self._task_group.start_soon(self.publish_local, event)

        async def unsubscribe() -> None:
            if not conn.is_closed():
                with move_on_after(3, shield=True):
                    await conn.remove_listener(self.channel, listen_callback)

        task_started_sent = False
        with receive:
            while True:
                async with AsyncExitStack() as exit_stack:
                    conn = await exit_stack.enter_async_context(self._connect())
                    self._logger.info("Connection established")
                    try:
                        await conn.add_listener(self.channel, listen_callback)
                        exit_stack.push_async_callback(unsubscribe)
                        if not task_started_sent:
                            task_status.started()
                            task_started_sent = True

                        while True:
                            notification: str | None = None
                            with move_on_after(self.max_idle_time):
                                try:
                                    notification = await receive.receive()
                                except EndOfStream:
                                    self._logger.info("Stream finished")
                                    return

                            if notification:
                                await conn.execute(
                                    "SELECT pg_notify($1, $2)",
                                    self.channel,
                                    notification,
                                )
                            else:
                                await conn.execute("SELECT 1")
                    except InterfaceError as exc:
                        self._logger.error("Connection error: %s", exc)

    async def publish(self, event: Event) -> None:
        notification = self.generate_notification_str(event)
        if len(notification) > 7999:
            raise SerializationError(
                "Serialized event object exceeds 7999 bytes in size"
            )

        await self._send.send(notification)



================================================
FILE: src/apscheduler/eventbrokers/base.py
================================================
from __future__ import annotations

from base64 import b64decode, b64encode
from collections.abc import Iterable
from contextlib import AsyncExitStack
from inspect import iscoroutine
from logging import Logger
from typing import Any, Callable

import attrs
from anyio import CapacityLimiter, create_task_group, to_thread
from anyio.abc import TaskGroup

from .. import _events
from .._events import Event
from .._exceptions import DeserializationError
from .._retry import RetryMixin
from ..abc import EventBroker, Serializer, Subscription
from ..serializers.json import JSONSerializer


@attrs.define(eq=False, frozen=True)
class LocalSubscription(Subscription):
    callback: Callable[[Event], Any]
    event_types: set[type[Event]] | None
    one_shot: bool
    is_async: bool
    token: object
    _source: BaseEventBroker

    def unsubscribe(self) -> None:
        self._source.unsubscribe(self.token)


@attrs.define(kw_only=True)
class BaseEventBroker(EventBroker):
    _logger: Logger = attrs.field(init=False)
    _subscriptions: dict[object, LocalSubscription] = attrs.field(
        init=False, factory=dict
    )
    _task_group: TaskGroup = attrs.field(init=False)
    _thread_limiter: CapacityLimiter = attrs.field(init=False)

    async def start(self, exit_stack: AsyncExitStack, logger: Logger) -> None:
        self._logger = logger
        self._task_group = await exit_stack.enter_async_context(create_task_group())
        self._thread_limiter = CapacityLimiter(1)

    def subscribe(
        self,
        callback: Callable[[Event], Any],
        event_types: Iterable[type[Event]] | None = None,
        *,
        is_async: bool = True,
        one_shot: bool = False,
    ) -> Subscription:
        types = set(event_types) if event_types else None
        token = object()
        subscription = LocalSubscription(
            callback, types, one_shot, is_async, token, self
        )
        self._subscriptions[token] = subscription
        return subscription

    def unsubscribe(self, token: object) -> None:
        self._subscriptions.pop(token, None)

    async def publish_local(self, event: Event) -> None:
        event_type = type(event)
        one_shot_tokens: list[object] = []
        for _token, subscription in self._subscriptions.items():
            if (
                subscription.event_types is None
                or event_type in subscription.event_types
            ):
                self._task_group.start_soon(self._deliver_event, subscription, event)
                if subscription.one_shot:
                    one_shot_tokens.append(subscription.token)

        for token in one_shot_tokens:
            self.unsubscribe(token)

    async def _deliver_event(
        self, subscription: LocalSubscription, event: Event
    ) -> None:
        try:
            if subscription.is_async:
                retval = subscription.callback(event)
                if iscoroutine(retval):
                    await retval
            else:
                await to_thread.run_sync(
                    subscription.callback, event, limiter=self._thread_limiter
                )
        except Exception:
            self._logger.exception(
                "Error delivering %s event", event.__class__.__name__
            )


@attrs.define(kw_only=True)
class BaseExternalEventBroker(BaseEventBroker, RetryMixin):
    """
    Base class for event brokers that use an external service.

    :param serializer: the serializer used to (de)serialize events for transport
    """

    serializer: Serializer = attrs.field(factory=JSONSerializer)

    def generate_notification(self, event: Event) -> bytes:
        serialized = self.serializer.serialize(event.marshal())
        return event.__class__.__name__.encode("ascii") + b" " + serialized

    def generate_notification_str(self, event: Event) -> str:
        serialized = self.serializer.serialize(event.marshal())
        return event.__class__.__name__ + " " + b64encode(serialized).decode("ascii")

    def _reconstitute_event(self, event_type: str, serialized: bytes) -> Event | None:
        try:
            kwargs = self.serializer.deserialize(serialized)
        except DeserializationError:
            self._logger.exception(
                "Failed to deserialize an event of type %s",
                event_type,
                extra={"serialized": serialized},
            )
            return None

        try:
            event_class = getattr(_events, event_type)
        except AttributeError:
            self._logger.error(
                "Receive notification for a nonexistent event type: %s",
                event_type,
                extra={"serialized": serialized},
            )
            return None

        try:
            return event_class.unmarshal(kwargs)
        except Exception:
            self._logger.exception("Error reconstituting event of type %s", event_type)
            return None

    def reconstitute_event(self, payload: bytes) -> Event | None:
        try:
            event_type_bytes, serialized = payload.split(b" ", 1)
        except ValueError:
            self._logger.error(
                "Received malformatted notification", extra={"payload": payload}
            )
            return None

        event_type = event_type_bytes.decode("ascii", errors="replace")
        return self._reconstitute_event(event_type, serialized)

    def reconstitute_event_str(self, payload: str) -> Event | None:
        try:
            event_type, b64_serialized = payload.split(" ", 1)
        except ValueError:
            self._logger.error(
                "Received malformatted notification", extra={"payload": payload}
            )
            return None

        return self._reconstitute_event(event_type, b64decode(b64_serialized))



================================================
FILE: src/apscheduler/eventbrokers/local.py
================================================
from __future__ import annotations

import attrs

from .._events import Event
from .._utils import create_repr
from .base import BaseEventBroker


@attrs.define(eq=False, repr=False)
class LocalEventBroker(BaseEventBroker):
    """
    Asynchronous, local event broker.

    This event broker only broadcasts within the process it runs in, and is therefore
    not suitable for multi-node or multiprocess use cases.

    Does not serialize events.
    """

    def __repr__(self) -> str:
        return create_repr(self)

    async def publish(self, event: Event) -> None:
        await self.publish_local(event)



================================================
FILE: src/apscheduler/eventbrokers/mqtt.py
================================================
from __future__ import annotations

import sys
from concurrent.futures import Future
from contextlib import AsyncExitStack
from logging import Logger
from ssl import SSLContext
from typing import Any

import attrs
from anyio import to_thread
from anyio.from_thread import BlockingPortal
from attr.validators import in_, instance_of, optional
from paho.mqtt.client import Client, MQTTMessage
from paho.mqtt.enums import CallbackAPIVersion

from .._events import Event
from .._utils import create_repr
from .base import BaseExternalEventBroker

ALLOWED_TRANSPORTS = ("mqtt", "mqtts", "ws", "wss", "unix")


@attrs.define(eq=False, repr=False)
class MQTTEventBroker(BaseExternalEventBroker):
    """
    An event broker that uses an MQTT (v3.1 or v5) broker to broadcast events.

    Requires the paho-mqtt_ library (v2.0 or later) to be installed.

    .. _paho-mqtt: https://pypi.org/project/paho-mqtt/

    :param host: MQTT broker host (or UNIX socket path)
    :param port: MQTT broker port (for ``tcp`` or ``websocket`` transports)
    :param transport: one of ``tcp``, ``websocket`` or ``unix`` (default: ``tcp``)
    :param client_id: MQTT client ID (needed to resume an MQTT session if a connection
        is broken)
    :param ssl: either ``True`` or a custom SSL context to enable SSL/TLS, ``False`` to
        disable
    :param topic: topic on which to send the messages
    :param subscribe_qos: MQTT QoS to use for subscribing messages
    :param publish_qos: MQTT QoS to use for publishing messages
    """

    host: str = attrs.field(default="localhost", validator=instance_of(str))
    port: int | None = attrs.field(default=None, validator=optional(instance_of(int)))
    transport: str = attrs.field(
        default="tcp", validator=in_(["tcp", "websocket", "unix"])
    )
    client_id: str | None = attrs.field(
        default=None, validator=optional(instance_of(str))
    )
    ssl: bool | SSLContext = attrs.field(
        default=False, validator=instance_of((bool, SSLContext))
    )
    topic: str = attrs.field(
        kw_only=True, default="apscheduler", validator=instance_of(str)
    )
    subscribe_qos: int = attrs.field(kw_only=True, default=0, validator=in_([0, 1, 2]))
    publish_qos: int = attrs.field(kw_only=True, default=0, validator=in_([0, 1, 2]))

    _use_tls: bool = attrs.field(init=False, default=False)
    _client: Client = attrs.field(init=False)
    _portal: BlockingPortal = attrs.field(init=False)
    _ready_future: Future[None] = attrs.field(init=False)

    def __attrs_post_init__(self) -> None:
        if self.port is None:
            if self.transport == "tcp":
                self.port = 8883 if self.ssl else 1883
            elif self.transport == "websocket":
                self.port = 443 if self.ssl else 80

        self._client = Client(
            callback_api_version=CallbackAPIVersion.VERSION2,
            client_id=self.client_id,
            transport=self.transport,
        )
        if isinstance(self.ssl, SSLContext):
            self._client.tls_set_context(self.ssl)
        elif self.ssl:
            self._client.tls_set()

    def __repr__(self) -> str:
        return create_repr(self, "host", "port", "transport")

    async def start(self, exit_stack: AsyncExitStack, logger: Logger) -> None:
        await super().start(exit_stack, logger)
        self._portal = await exit_stack.enter_async_context(BlockingPortal())
        self._ready_future = Future()
        self._client.on_connect = self._on_connect
        self._client.on_connect_fail = self._on_connect_fail
        self._client.on_disconnect = self._on_disconnect
        self._client.on_message = self._on_message
        self._client.on_subscribe = self._on_subscribe
        self._client.connect_async(self.host, self.port)
        self._client.loop_start()
        exit_stack.push_async_callback(to_thread.run_sync, self._client.loop_stop)

        # Wait for the connection attempt to be done
        await to_thread.run_sync(self._ready_future.result, 10)

        # Schedule a disconnection for when the exit stack is exited
        exit_stack.callback(self._client.disconnect)

    def _on_connect(self, client: Client, *_: Any) -> None:
        self._logger.info("%s: Connected", self.__class__.__name__)
        try:
            client.subscribe(self.topic, qos=self.subscribe_qos)
        except Exception as exc:
            self._ready_future.set_exception(exc)
            raise

    def _on_connect_fail(self, *_: Any) -> None:
        exc = sys.exc_info()[1]
        self._logger.error("%s: Connection failed (%s)", self.__class__.__name__, exc)

    def _on_disconnect(self, *args: Any) -> None:
        reason_code = args[3] if len(args) == 5 else args[2]
        self._logger.error(
            "%s: Disconnected (code: %s)", self.__class__.__name__, reason_code
        )

    def _on_subscribe(self, *_: Any) -> None:
        self._logger.info("%s: Subscribed", self.__class__.__name__)
        self._ready_future.set_result(None)

    def _on_message(self, _: Any, __: Any, msg: MQTTMessage) -> None:
        event = self.reconstitute_event(msg.payload)
        if event is not None:
            self._portal.call(self.publish_local, event)

    async def publish(self, event: Event) -> None:
        notification = self.generate_notification(event)
        await to_thread.run_sync(
            lambda: self._client.publish(self.topic, notification, qos=self.publish_qos)
        )



================================================
FILE: src/apscheduler/eventbrokers/psycopg.py
================================================
from __future__ import annotations

from collections.abc import AsyncGenerator, Mapping
from contextlib import AsyncExitStack, asynccontextmanager
from logging import Logger
from typing import TYPE_CHECKING, Any

import attrs
from anyio import (
    EndOfStream,
    create_memory_object_stream,
    move_on_after,
)
from anyio.abc import TaskStatus
from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream
from attr.validators import instance_of
from psycopg import AsyncConnection, InterfaceError

from .._events import Event
from .._exceptions import SerializationError
from .._utils import create_repr
from .._validators import positive_number
from .base import BaseExternalEventBroker

if TYPE_CHECKING:
    from sqlalchemy.ext.asyncio import AsyncEngine


def convert_options(value: Mapping[str, Any]) -> dict[str, Any]:
    return dict(value, autocommit=True)


@attrs.define(eq=False, repr=False)
class PsycopgEventBroker(BaseExternalEventBroker):
    """
    An asynchronous, psycopg_ based event broker that uses a PostgreSQL server to
    broadcast events using its ``NOTIFY`` mechanism.

    .. _psycopg: https://pypi.org/project/psycopg/

    :param conninfo: a libpq connection string (e.g.
        ``postgres://user:pass@host:port/dbname``)
    :param options: extra keyword arguments passed to
        :meth:`psycopg.AsyncConnection.connect`
    :param channel: the ``NOTIFY`` channel to use
    :param max_idle_time: maximum time (in seconds) to let the connection go idle,
        before sending a ``SELECT 1`` query to prevent a connection timeout
    """

    conninfo: str = attrs.field(validator=instance_of(str))
    options: Mapping[str, Any] = attrs.field(
        factory=dict, converter=convert_options, validator=instance_of(Mapping)
    )
    channel: str = attrs.field(
        kw_only=True, default="apscheduler", validator=instance_of(str)
    )
    max_idle_time: float = attrs.field(
        kw_only=True, default=10, validator=[instance_of((int, float)), positive_number]
    )

    _send: MemoryObjectSendStream[str] = attrs.field(init=False)

    @classmethod
    def from_async_sqla_engine(
        cls,
        engine: AsyncEngine,
        options: Mapping[str, Any] | None = None,
        **kwargs: Any,
    ) -> PsycopgEventBroker:
        """
        Create a new psycopg event broker from a SQLAlchemy engine.

        The engine will only be used to create the appropriate options for
        :meth:`psycopg.AsyncConnection.connect`.

        :param engine: an asynchronous SQLAlchemy engine using psycopg as the driver
        :type engine: ~sqlalchemy.ext.asyncio.AsyncEngine
        :param options: extra keyword arguments passed to
            :meth:`psycopg.AsyncConnection.connect`
        :param kwargs: keyword arguments to pass to the initializer of this class
        :return: the newly created event broker

        """
        if engine.dialect.driver != "psycopg":
            raise ValueError(
                f'The driver in the engine must be "psycopg" (current: '
                f"{engine.dialect.driver})"
            )

        conninfo = engine.url.render_as_string(hide_password=False).replace(
            "+psycopg", ""
        )
        return cls(conninfo, options or {}, **kwargs)

    def __repr__(self) -> str:
        return create_repr(self, "conninfo")

    @property
    def _temporary_failure_exceptions(self) -> tuple[type[Exception], ...]:
        return OSError, InterfaceError

    @asynccontextmanager
    async def _connect(self) -> AsyncGenerator[AsyncConnection, None]:
        async for attempt in self._retry():
            with attempt:
                conn = await AsyncConnection.connect(self.conninfo, **self.options)
                try:
                    yield conn
                finally:
                    with move_on_after(5, shield=True):
                        await conn.close()

    async def start(self, exit_stack: AsyncExitStack, logger: Logger) -> None:
        await super().start(exit_stack, logger)
        self._send, receive = create_memory_object_stream[str](100)
        try:
            await exit_stack.enter_async_context(self._send)
            await self._task_group.start(self._listen_notifications)
            exit_stack.callback(self._task_group.cancel_scope.cancel)
            await self._task_group.start(self._publish_notifications, receive)
        except BaseException:
            receive.close()
            raise

    async def _listen_notifications(self, *, task_status: TaskStatus[None]) -> None:
        task_started_sent = False
        while True:
            async with self._connect() as conn:
                try:
                    await conn.execute(f"LISTEN {self.channel}")

                    if not task_started_sent:
                        task_status.started()
                        task_started_sent = True

                    self._logger.debug("Listen connection established")
                    async for notify in conn.notifies():
                        if event := self.reconstitute_event_str(notify.payload):
                            await self.publish_local(event)
                except InterfaceError as exc:
                    self._logger.error("Connection error: %s", exc)

    async def _publish_notifications(
        self, receive: MemoryObjectReceiveStream[str], *, task_status: TaskStatus[None]
    ) -> None:
        task_started_sent = False
        with receive:
            while True:
                async with self._connect() as conn:
                    if not task_started_sent:
                        task_status.started()
                        task_started_sent = True

                    self._logger.debug("Publish connection established")
                    notification: str | None = None
                    while True:
                        with move_on_after(self.max_idle_time):
                            try:
                                notification = await receive.receive()
                            except EndOfStream:
                                return

                        if notification:
                            await conn.execute(
                                "SELECT pg_notify(%t, %t)", [self.channel, notification]
                            )
                        else:
                            await conn.execute("SELECT 1")

    async def publish(self, event: Event) -> None:
        notification = self.generate_notification_str(event)
        if len(notification) > 7999:
            raise SerializationError(
                "Serialized event object exceeds 7999 bytes in size"
            )

        await self._send.send(notification)



================================================
FILE: src/apscheduler/eventbrokers/redis.py
================================================
from __future__ import annotations

from asyncio import CancelledError
from contextlib import AsyncExitStack
from logging import Logger

import anyio
import attrs
import tenacity
from anyio import move_on_after
from attr.validators import instance_of
from redis import ConnectionError
from redis.asyncio import Redis
from redis.asyncio.client import PubSub
from redis.asyncio.connection import ConnectionPool

from .._events import Event
from .._utils import create_repr
from .base import BaseExternalEventBroker


@attrs.define(eq=False, repr=False)
class RedisEventBroker(BaseExternalEventBroker):
    """
    An event broker that uses a Redis server to broadcast events.

    Requires the redis_ library to be installed.

    .. _redis: https://pypi.org/project/redis/

    :param client_or_url: an asynchronous Redis client or a Redis URL
        (```redis://...```)
    :param channel: channel on which to send the messages
    :param stop_check_interval: interval (in seconds) on which the channel listener
        should check if it should stop (higher values mean slower reaction time but less
        CPU use)

    .. note:: The event broker will not manage the life cycle of any client instance
        passed to it, so you need to close the client afterwards when you're done with
        it.
    """

    client_or_url: Redis | str = attrs.field(validator=instance_of((Redis, str)))
    channel: str = attrs.field(kw_only=True, default="apscheduler")
    stop_check_interval: float = attrs.field(kw_only=True, default=1)

    _client: Redis = attrs.field(init=False)
    _close_on_exit: bool = attrs.field(init=False, default=False)
    _stopped: bool = attrs.field(init=False, default=True)

    def __attrs_post_init__(self) -> None:
        if isinstance(self.client_or_url, str):
            pool = ConnectionPool.from_url(self.client_or_url)
            self._client = Redis(connection_pool=pool)
            self._close_on_exit = True
        else:
            self._client = self.client_or_url

    def __repr__(self) -> str:
        return create_repr(self, "client_or_url")

    def _retry(self) -> tenacity.AsyncRetrying:
        def after_attempt(retry_state: tenacity.RetryCallState) -> None:
            self._logger.warning(
                "%s: connection failure (attempt %d): %s",
                self.__class__.__name__,
                retry_state.attempt_number,
                retry_state.outcome.exception(),
            )

        return tenacity.AsyncRetrying(
            stop=self.retry_settings.stop,
            wait=self.retry_settings.wait,
            retry=tenacity.retry_if_exception_type(ConnectionError),
            after=after_attempt,
            sleep=anyio.sleep,
            reraise=True,
        )

    async def _close_client(self) -> None:
        with move_on_after(5, shield=True):
            await self._client.aclose(close_connection_pool=True)

    async def start(self, exit_stack: AsyncExitStack, logger: Logger) -> None:
        # Close the client and its connection pool if this broker was created using
        # .from_url()
        if self._close_on_exit:
            exit_stack.push_async_callback(self._close_client)

        pubsub = await exit_stack.enter_async_context(self._client.pubsub())
        await pubsub.subscribe(self.channel)
        await super().start(exit_stack, logger)

        self._stopped = False
        exit_stack.callback(setattr, self, "_stopped", True)
        self._task_group.start_soon(
            self._listen_messages, pubsub, name="Redis subscriber"
        )

    async def _listen_messages(self, pubsub: PubSub) -> None:
        while not self._stopped:
            try:
                async for attempt in self._retry():
                    with attempt:
                        msg = await pubsub.get_message(
                            ignore_subscribe_messages=True,
                            timeout=self.stop_check_interval,
                        )

                if msg and isinstance(msg["data"], bytes):
                    event = self.reconstitute_event(msg["data"])
                    if event is not None:
                        await self.publish_local(event)
            except Exception as exc:
                # CancelledError is a subclass of Exception in Python 3.7
                if not isinstance(exc, CancelledError):
                    self._logger.exception(
                        "%s listener crashed", self.__class__.__name__
                    )

                await pubsub.aclose()
                raise

    async def publish(self, event: Event) -> None:
        notification = self.generate_notification(event)
        async for attempt in self._retry():
            with attempt:
                await self._client.publish(self.channel, notification)


